{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Generator Coroutines\n",
    "\n",
    "Stateful generators\n",
    "<created>05/25/25</created>\n",
    "<updated></updated>"
   ],
   "id": "85217407509a04ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "A coroutine is a special type of generator that can receive values from the caller. This allows for more complex interactions between the generator and the caller, enabling the generator to act as a state machine or to maintain state across multiple calls.\n",
    "\n",
    "In practice this is achieved by using the `yield` statement in a function, which allows the function to pause its execution and yield control back to the caller. The caller can then send values back to the generator using the `send()` method, which resumes the generator's execution at the point where it was paused.\n",
    "\n",
    "Note that the generator must be 'primed' first. Once the function is called, call `next()` on the generator to advance it to the first `yield` statement. After that, you can use `send()` to send values back into the generator."
   ],
   "id": "c42c28e3eef1c63d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here's a contrived example of a coroutine that processes characters from a string and yields them back to the caller\n",
    "\n",
    "Something to keep in mind, is that nothing is executed inside `stream_characters()` until the first `next()` is called. Once the first `next()` is called, the function will run until it hits the first `yield` statement.\n",
    "\n",
    "This is where we have to think differently about the `yield` statement. The first `yield` statement is where the function will pause and wait for a value to be sent back into it. This is the point where we can send a value back into the generator using the `send()` method."
   ],
   "id": "cd0b45cb728675c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T19:07:03.507884Z",
     "start_time": "2025-05-25T19:07:03.505298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def stream_characters():\n",
    "    print(\"Generator started\")\n",
    "    text = yield\n",
    "    print(\"Generator resumed with text:\", text)\n",
    "    yield\n",
    "    print(\"Generator acknowledged text:\", text)\n",
    "    if text is not None:\n",
    "        for char in text:\n",
    "            yield char\n",
    "\n",
    "\n",
    "gen = stream_characters()\n",
    "next(gen)\n",
    "gen.send(\"Hello World\")  # Send a string to the generator\n",
    "for char in gen:\n",
    "    print(\"Received character:\", char)\n"
   ],
   "id": "95f6ecd333c81894",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator started\n",
      "Generator resumed with text: Hello World\n",
      "Generator acknowledged text: Hello World\n",
      "Received character: H\n",
      "Received character: e\n",
      "Received character: l\n",
      "Received character: l\n",
      "Received character: o\n",
      "Received character:  \n",
      "Received character: W\n",
      "Received character: o\n",
      "Received character: r\n",
      "Received character: l\n",
      "Received character: d\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### A More Practical Example\n",
    "\n",
    "What's interesting about this is that we can treat these generators somewhat like a thread. We can send messages to them, and they can send messages back to us. They *can* share state, but for practicality, we should avoid that. In fact, it's best to keep the state inside the generator and not share it with the caller. This way, we can keep the generator as a pure function.\n",
    "\n",
    "For this example, let's create a coroutine that simulates an LLM conversation. What we want to do is maintain one or more conversations and not need to worry about the state of each conversation in the caller. The generator will maintain the state of the conversation and return the conversation history when requested. We're not going to use any LLMs here, but we will simulate conversation history.\n",
    "\n",
    "While we're at it, let's make a decorator that will automatically prime the coroutine for us. This will allow us to use the coroutine without having to call `next()` on it first."
   ],
   "id": "6d96f62a96a0568c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T19:07:03.611608Z",
     "start_time": "2025-05-25T19:07:03.608829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Literal, Counter\n",
    "from functools import wraps\n",
    "from operator import methodcaller\n",
    "\n",
    "def coroutine(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        gen = func(*args, **kwargs)\n",
    "        next(gen)  # Prime the coroutine\n",
    "        return gen\n",
    "    return wrapper\n",
    "\n",
    "@coroutine\n",
    "def conversation(mode: Literal[\"lower\", \"upper\"]):\n",
    "    # The conversation history is maintained in the generator\n",
    "    history = []\n",
    "    token_counts = Counter()\n",
    "    user_message = yield # Received via .send()\n",
    "    transformer = methodcaller(mode)\n",
    "    while 1:\n",
    "        if user_message is None: # End of conversation, return the history and token counts\n",
    "            yield history, token_counts.most_common()\n",
    "            return\n",
    "        # Echo the user message in all caps\n",
    "        token_counts.update(user_message.split())\n",
    "        llm_message = transformer(user_message)\n",
    "        token_counts.update(llm_message.split())\n",
    "        history.extend([{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_message,\n",
    "        },\n",
    "            {\n",
    "            \"role\": \"llm\",\n",
    "            \"content\": llm_message,\n",
    "        }])\n",
    "\n",
    "        user_message = yield llm_message\n"
   ],
   "id": "7132ad7c637aa8a9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T19:07:03.656343Z",
     "start_time": "2025-05-25T19:07:03.654456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's maintain 2 conversations\n",
    "convo_lower = conversation(\"lower\")\n",
    "convo_upper = conversation(\"upper\")\n",
    "\n",
    "# Send a message to the lower case conversation\n",
    "response = convo_lower.send(\"Hello World\")\n",
    "\n",
    "# If we don't care about the response, we can just call send() again\n",
    "for i in range(5):\n",
    "    convo_lower.send(f\"Hello World {i}\")\n",
    "\n",
    "# Send a message to the upper case conversation\n",
    "response = convo_upper.send(\"Hello World\")\n",
    "# If we don't care about the response, we can just call send() again\n",
    "for i in range(5):\n",
    "    convo_upper.send(f\"Hello World {i}\")"
   ],
   "id": "749f84e84a43c862",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T19:07:03.701268Z",
     "start_time": "2025-05-25T19:07:03.699529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now we can end the conversations and get the history and token counts\n",
    "history_lower, token_counts_lower = convo_lower.send(None)\n",
    "print(history_lower)\n",
    "print(token_counts_lower)"
   ],
   "id": "9f32db399b9a0dfc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Hello World'}, {'role': 'llm', 'content': 'hello world'}, {'role': 'user', 'content': 'Hello World 0'}, {'role': 'llm', 'content': 'hello world 0'}, {'role': 'user', 'content': 'Hello World 1'}, {'role': 'llm', 'content': 'hello world 1'}, {'role': 'user', 'content': 'Hello World 2'}, {'role': 'llm', 'content': 'hello world 2'}, {'role': 'user', 'content': 'Hello World 3'}, {'role': 'llm', 'content': 'hello world 3'}, {'role': 'user', 'content': 'Hello World 4'}, {'role': 'llm', 'content': 'hello world 4'}]\n",
      "[('Hello', 6), ('World', 6), ('hello', 6), ('world', 6), ('0', 2), ('1', 2), ('2', 2), ('3', 2), ('4', 2)]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T19:07:03.747196Z",
     "start_time": "2025-05-25T19:07:03.745435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "history_upper, token_counts_upper = convo_upper.send(None)\n",
    "print(history_upper)\n",
    "print(token_counts_upper)"
   ],
   "id": "fd90af74864cf57c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Hello World'}, {'role': 'llm', 'content': 'HELLO WORLD'}, {'role': 'user', 'content': 'Hello World 0'}, {'role': 'llm', 'content': 'HELLO WORLD 0'}, {'role': 'user', 'content': 'Hello World 1'}, {'role': 'llm', 'content': 'HELLO WORLD 1'}, {'role': 'user', 'content': 'Hello World 2'}, {'role': 'llm', 'content': 'HELLO WORLD 2'}, {'role': 'user', 'content': 'Hello World 3'}, {'role': 'llm', 'content': 'HELLO WORLD 3'}, {'role': 'user', 'content': 'Hello World 4'}, {'role': 'llm', 'content': 'HELLO WORLD 4'}]\n",
      "[('Hello', 6), ('World', 6), ('HELLO', 6), ('WORLD', 6), ('0', 2), ('1', 2), ('2', 2), ('3', 2), ('4', 2)]\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
